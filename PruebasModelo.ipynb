{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = 'bolt://localhost:7687'\n",
    "user = 'neo4j'\n",
    "psw = 'password'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('data/x_train.csv')\n",
    "y_train = pd.read_csv('data/y_train.csv')\n",
    "x_val = pd.read_csv('data/x_val.csv')\n",
    "y_val = pd.read_csv('data/y_val.csv')\n",
    "x_test = pd.read_csv('data/x_test.csv')\n",
    "y_test = pd.read_csv('data/y_test.csv')\n",
    "features = ['cat_based', 'cat_cnt', 'user_based', 'user_cnt',\n",
    "            'adamic_adar', 'resource_allocation', 'link_cnt', 'cat_avg', 'user_avg',\n",
    "            'adar_avg', 'ra_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(706, 151, 152)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train), len(y_val), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [i for i in range(706)]\n",
    "val = [706 + i for i in range(151)]\n",
    "test = [856 + i for i in range(152)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>podcast_id</th>\n",
       "      <th>cat_based</th>\n",
       "      <th>cat_cnt</th>\n",
       "      <th>user_based</th>\n",
       "      <th>user_cnt</th>\n",
       "      <th>adamic_adar</th>\n",
       "      <th>resource_allocation</th>\n",
       "      <th>link_cnt</th>\n",
       "      <th>cat_avg</th>\n",
       "      <th>user_avg</th>\n",
       "      <th>adar_avg</th>\n",
       "      <th>ra_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>426DDBF8D527E3A</td>\n",
       "      <td>b69677f9f694bd3566eaf9fff0343784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E2FC7EABA5761EC</td>\n",
       "      <td>b5cf7d57fdf09b9e4177a0afb8cd467f</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.998319</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.599664</td>\n",
       "      <td>0.143529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AC072D3BF1CB749</td>\n",
       "      <td>b4c3c3ebdd76e284f7d9fa358ac82030</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.440813</td>\n",
       "      <td>2.436210</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.220406</td>\n",
       "      <td>1.218105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5C82AAF7008477B</td>\n",
       "      <td>b1a3eb2aa8e82ecbe9c91ed9a963c362</td>\n",
       "      <td>60.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>26.122381</td>\n",
       "      <td>4.198098</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.009414</td>\n",
       "      <td>0.322931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5D7ADA74FE6E018</td>\n",
       "      <td>dfa087abaf0dd92207c499a9ed5e4451</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.104152</td>\n",
       "      <td>1.361685</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.420830</td>\n",
       "      <td>0.272337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           user_id                        podcast_id  cat_based  cat_cnt  \\\n",
       "0  426DDBF8D527E3A  b69677f9f694bd3566eaf9fff0343784        0.0      0.0   \n",
       "1  E2FC7EABA5761EC  b5cf7d57fdf09b9e4177a0afb8cd467f        7.0      7.0   \n",
       "2  AC072D3BF1CB749  b4c3c3ebdd76e284f7d9fa358ac82030       10.0      2.0   \n",
       "3  5C82AAF7008477B  b1a3eb2aa8e82ecbe9c91ed9a963c362       60.0     16.0   \n",
       "4  5D7ADA74FE6E018  dfa087abaf0dd92207c499a9ed5e4451       20.0      4.0   \n",
       "\n",
       "   user_based  user_cnt  adamic_adar  resource_allocation  link_cnt  cat_avg  \\\n",
       "0         0.0       0.0     0.000000             0.000000       5.0     0.00   \n",
       "1         0.0       0.0     2.998319             0.717647       5.0     1.00   \n",
       "2        45.0       3.0    10.440813             2.436210       2.0     5.00   \n",
       "3        52.0       4.0    26.122381             4.198098      13.0     3.75   \n",
       "4         0.0       0.0     7.104152             1.361685       5.0     5.00   \n",
       "\n",
       "   user_avg  adar_avg    ra_avg  \n",
       "0       0.0  0.000000  0.000000  \n",
       "1       0.0  0.599664  0.143529  \n",
       "2      15.0  5.220406  1.218105  \n",
       "3      13.0  2.009414  0.322931  \n",
       "4       0.0  1.420830  0.272337  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([x_train, x_val, x_test])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat_based              1322.000000\n",
       "cat_cnt                 282.000000\n",
       "user_based             6443.000000\n",
       "user_cnt                443.000000\n",
       "adamic_adar            1952.829911\n",
       "resource_allocation     575.056886\n",
       "link_cnt                291.000000\n",
       "cat_avg                 278.892857\n",
       "user_avg                708.033333\n",
       "adar_avg                829.161883\n",
       "ra_avg                  263.489960\n",
       "dtype: float64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_df = df.groupby('user_id')[features].max()\n",
    "max_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_based</th>\n",
       "      <th>cat_cnt</th>\n",
       "      <th>user_based</th>\n",
       "      <th>user_cnt</th>\n",
       "      <th>adamic_adar</th>\n",
       "      <th>resource_allocation</th>\n",
       "      <th>link_cnt</th>\n",
       "      <th>cat_avg</th>\n",
       "      <th>user_avg</th>\n",
       "      <th>adar_avg</th>\n",
       "      <th>ra_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cat_based  cat_cnt  user_based  user_cnt  adamic_adar  \\\n",
       "689        inf      inf         NaN       NaN          inf   \n",
       "139        inf      inf         NaN       NaN          inf   \n",
       "\n",
       "     resource_allocation  link_cnt  cat_avg  user_avg  adar_avg  ra_avg  \n",
       "689                  inf       inf      inf       NaN       inf     inf  \n",
       "139                  inf       inf      inf       NaN       inf     inf  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['user_id'] == '14F47B6A810BA33'][features]/0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def norm_row(row, max_df):\n",
    "    try:\n",
    "        return row[features].values / max_df.loc[row['user_id']].values\n",
    "    except:\n",
    "        return [0] * len(features)\n",
    "\n",
    "def norm_by_user(df, max_df):\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df[features] = df.apply(lambda row: norm_row(row, max_df), axis = 1, result_type='expand')\n",
    "    return new_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_based</th>\n",
       "      <th>cat_cnt</th>\n",
       "      <th>user_based</th>\n",
       "      <th>user_cnt</th>\n",
       "      <th>adamic_adar</th>\n",
       "      <th>resource_allocation</th>\n",
       "      <th>link_cnt</th>\n",
       "      <th>cat_avg</th>\n",
       "      <th>user_avg</th>\n",
       "      <th>adar_avg</th>\n",
       "      <th>ra_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1009.000000</td>\n",
       "      <td>1009.000000</td>\n",
       "      <td>1009.000000</td>\n",
       "      <td>1009.000000</td>\n",
       "      <td>1009.000000</td>\n",
       "      <td>1009.000000</td>\n",
       "      <td>1009.000000</td>\n",
       "      <td>1009.000000</td>\n",
       "      <td>1009.000000</td>\n",
       "      <td>1009.000000</td>\n",
       "      <td>1009.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.443932</td>\n",
       "      <td>0.482266</td>\n",
       "      <td>0.367770</td>\n",
       "      <td>0.382974</td>\n",
       "      <td>0.430134</td>\n",
       "      <td>0.393652</td>\n",
       "      <td>0.658077</td>\n",
       "      <td>0.524905</td>\n",
       "      <td>0.460782</td>\n",
       "      <td>0.430134</td>\n",
       "      <td>0.393652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.415557</td>\n",
       "      <td>0.432655</td>\n",
       "      <td>0.420172</td>\n",
       "      <td>0.435045</td>\n",
       "      <td>0.407566</td>\n",
       "      <td>0.405149</td>\n",
       "      <td>0.474589</td>\n",
       "      <td>0.460159</td>\n",
       "      <td>0.467875</td>\n",
       "      <td>0.407566</td>\n",
       "      <td>0.405149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.042963</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.426870</td>\n",
       "      <td>0.247035</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.426870</td>\n",
       "      <td>0.247035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.850000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.868866</td>\n",
       "      <td>0.849875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.868866</td>\n",
       "      <td>0.849875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         cat_based      cat_cnt   user_based     user_cnt  adamic_adar  \\\n",
       "count  1009.000000  1009.000000  1009.000000  1009.000000  1009.000000   \n",
       "mean      0.443932     0.482266     0.367770     0.382974     0.430134   \n",
       "std       0.415557     0.432655     0.420172     0.435045     0.407566   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.500000     0.555556     0.042963     0.044444     0.426870   \n",
       "75%       0.850000     1.000000     0.831461     1.000000     0.868866   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "       resource_allocation     link_cnt      cat_avg     user_avg  \\\n",
       "count          1009.000000  1009.000000  1009.000000  1009.000000   \n",
       "mean              0.393652     0.658077     0.524905     0.460782   \n",
       "std               0.405149     0.474589     0.460159     0.467875   \n",
       "min               0.000000     0.000000     0.000000     0.000000   \n",
       "25%               0.000000     0.000000     0.000000     0.000000   \n",
       "50%               0.247035     1.000000     0.800000     0.466667   \n",
       "75%               0.849875     1.000000     1.000000     1.000000   \n",
       "max               1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "          adar_avg       ra_avg  \n",
       "count  1009.000000  1009.000000  \n",
       "mean      0.430134     0.393652  \n",
       "std       0.407566     0.405149  \n",
       "min       0.000000     0.000000  \n",
       "25%       0.000000     0.000000  \n",
       "50%       0.426870     0.247035  \n",
       "75%       0.868866     0.849875  \n",
       "max       1.000000     1.000000  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = norm_by_user(df, max_df)\n",
    "norm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat_based              0.0\n",
       "cat_cnt                0.0\n",
       "user_based             0.0\n",
       "user_cnt               0.0\n",
       "adamic_adar            0.0\n",
       "resource_allocation    0.0\n",
       "link_cnt               0.0\n",
       "cat_avg                0.0\n",
       "user_avg               0.0\n",
       "adar_avg               0.0\n",
       "ra_avg                 0.0\n",
       "Name: 143, dtype: float64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.iloc[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm = norm.iloc[train].copy()\n",
    "x_val_norm = norm.iloc[val].copy()\n",
    "x_test_norm = norm.iloc[test].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm.to_csv('data/x_train_norm.csv', index=False)\n",
    "x_val_norm.to_csv('data/x_val_norm.csv', index=False)\n",
    "x_test_norm.to_csv('data/x_test_norm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_norm = pd.read_csv('data/x_train_norm.csv')\n",
    "x_val_norm = pd.read_csv('data/x_val_norm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=0.5)\n",
    "lr.fit(x_train_norm[features], y_train.values.reshape(1,-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(x_val_norm[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7665453269593439, 0.7814569536423841)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred == y_val.values).mean(), y_val.values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5, solver='liblinear')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='liblinear', C=0.5)\n",
    "lr.fit(x_train[features], y_train.values.reshape(1,-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.7814569536423841)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = lr.predict_proba(x_val[features])\n",
    "(pred == y_val.values).mean(), y_val.values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.9765784 , 0.83735914, 0.94817315, 0.97539038, 0.99715436,\n",
       "        0.99068868, 0.56446921, 0.77782803, 0.52439113, 0.99066613,\n",
       "        0.9677561 , 0.90470777, 0.99501969, 0.67328874, 0.99066613,\n",
       "        0.52116607, 0.99511287, 0.67328874, 0.89421599, 0.97419214,\n",
       "        0.52116607, 0.81395661, 0.97086448, 0.02672527, 0.99511287,\n",
       "        0.52116607, 0.67297358, 0.99489216, 0.52439113, 0.92863467,\n",
       "        0.55550799, 0.96569192, 0.89386238, 0.99999961, 0.72749088,\n",
       "        0.86758888, 0.9552558 , 0.97805946, 0.99362465, 0.99980085,\n",
       "        0.83735914, 0.9525641 , 0.98594446, 0.82032334, 0.98481781,\n",
       "        0.02622049, 0.64092466, 0.52116607, 0.99064606, 0.98235352,\n",
       "        0.60681072, 0.83735914, 0.99377093, 0.7437581 , 0.45039655,\n",
       "        0.91187788, 0.67782276, 0.68578058, 0.52439113, 0.11889323,\n",
       "        0.67328874, 0.52116607, 0.9436236 , 0.73302253, 0.93126535,\n",
       "        0.89421599, 0.85889565, 0.02622049, 0.70675434, 0.47793682,\n",
       "        0.96954189, 0.52116607, 0.97805946, 0.02622049, 0.82770073,\n",
       "        0.98168046, 0.97805946, 0.5575897 , 0.82790749, 0.99994264,\n",
       "        0.97805946, 0.52116607, 0.85890592, 0.94408193, 0.89386238,\n",
       "        0.78700284, 0.52439113, 0.9552558 , 0.89601298, 0.89386238,\n",
       "        0.60681072, 0.72159475, 0.54633667, 0.99684309, 0.52439113,\n",
       "        0.94436056, 0.65018856, 0.5575897 , 0.9743129 , 0.52116607,\n",
       "        0.99990901, 0.85889565, 0.99068868, 0.85889565, 0.9552558 ,\n",
       "        0.94436056, 0.92379849, 0.99066613, 0.98235352, 0.85889565,\n",
       "        0.97005597, 0.88652641, 0.99984298, 0.99064606, 0.60681072,\n",
       "        0.52439113, 0.99511287, 0.9552558 , 0.99980085, 0.81715717,\n",
       "        0.85889565, 1.        , 0.87757624, 0.94366671, 0.52439113,\n",
       "        0.02622049, 0.9997904 , 0.72159475, 0.92379849, 0.98235352,\n",
       "        0.97320953, 0.79844075, 0.89120777, 0.85889565, 0.68029038,\n",
       "        0.92379849, 0.56409711, 0.47309362, 0.92379849, 0.88287764,\n",
       "        0.97086448, 0.11889323, 0.94408193, 0.85889565, 0.8803659 ,\n",
       "        0.97009845, 0.95659235, 0.9525641 , 0.95230606, 0.92814145,\n",
       "        0.52116607]),\n",
       " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "         0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "         0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "         0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "         1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1]]))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[:,1], y_val.values.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9337748344370861"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7523011981478064, 0.7988668555240793)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = lr.predict(x_train[features])\n",
    "(pred == y_train.values).mean(), y_train.values.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    InputLayer(input_shape = (len(features), )),\n",
    "    Dense(32, activation='tanh'),\n",
    "    Dropout(0.7),\n",
    "    Dense(32, activation='tanh'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation=tf.keras.activations.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 32)                384       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,473\n",
      "Trainable params: 1,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "23/23 [==============================] - 1s 6ms/step - loss: 0.9681 - val_loss: 0.5960\n",
      "Epoch 2/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.8428 - val_loss: 0.4852\n",
      "Epoch 3/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6913 - val_loss: 0.4790\n",
      "Epoch 4/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.6389 - val_loss: 0.4938\n",
      "Epoch 5/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6235 - val_loss: 0.5130\n",
      "Epoch 6/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6122 - val_loss: 0.5168\n",
      "Epoch 7/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5957 - val_loss: 0.5258\n",
      "Epoch 8/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6086 - val_loss: 0.5309\n",
      "Epoch 9/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5475 - val_loss: 0.5231\n",
      "Epoch 10/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.5454 - val_loss: 0.5155\n",
      "Epoch 11/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5631 - val_loss: 0.5131\n",
      "Epoch 12/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5422 - val_loss: 0.5168\n",
      "Epoch 13/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.5514 - val_loss: 0.5126\n",
      "Epoch 14/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.5456 - val_loss: 0.5181\n",
      "Epoch 15/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.5290 - val_loss: 0.5181\n",
      "Epoch 16/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.5183 - val_loss: 0.5174\n",
      "Epoch 17/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.5225 - val_loss: 0.5197\n",
      "Epoch 18/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4860 - val_loss: 0.5122\n",
      "Epoch 19/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4977 - val_loss: 0.5099\n",
      "Epoch 20/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.5042 - val_loss: 0.5161\n",
      "Epoch 21/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.5278 - val_loss: 0.5019\n",
      "Epoch 22/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4972 - val_loss: 0.5004\n",
      "Epoch 23/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5048 - val_loss: 0.4906\n",
      "Epoch 24/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5031 - val_loss: 0.4874\n",
      "Epoch 25/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4732 - val_loss: 0.4772\n",
      "Epoch 26/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4682 - val_loss: 0.4769\n",
      "Epoch 27/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4794 - val_loss: 0.4811\n",
      "Epoch 28/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4665 - val_loss: 0.4769\n",
      "Epoch 29/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4782 - val_loss: 0.4659\n",
      "Epoch 30/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4735 - val_loss: 0.4662\n",
      "Epoch 31/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4693 - val_loss: 0.4753\n",
      "Epoch 32/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4655 - val_loss: 0.4656\n",
      "Epoch 33/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4608 - val_loss: 0.4576\n",
      "Epoch 34/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4860 - val_loss: 0.4619\n",
      "Epoch 35/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4701 - val_loss: 0.4642\n",
      "Epoch 36/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4378 - val_loss: 0.4591\n",
      "Epoch 37/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4666 - val_loss: 0.4566\n",
      "Epoch 38/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4731 - val_loss: 0.4522\n",
      "Epoch 39/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4514 - val_loss: 0.4505\n",
      "Epoch 40/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4556 - val_loss: 0.4428\n",
      "Epoch 41/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4554 - val_loss: 0.4443\n",
      "Epoch 42/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4553 - val_loss: 0.4448\n",
      "Epoch 43/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4444 - val_loss: 0.4439\n",
      "Epoch 44/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4377 - val_loss: 0.4421\n",
      "Epoch 45/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4224 - val_loss: 0.4365\n",
      "Epoch 46/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4143 - val_loss: 0.4315\n",
      "Epoch 47/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4362 - val_loss: 0.4398\n",
      "Epoch 48/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4374 - val_loss: 0.4388\n",
      "Epoch 49/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4258 - val_loss: 0.4295\n",
      "Epoch 50/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4257 - val_loss: 0.4260\n",
      "Epoch 51/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4160 - val_loss: 0.4278\n",
      "Epoch 52/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4277 - val_loss: 0.4173\n",
      "Epoch 53/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4291 - val_loss: 0.4326\n",
      "Epoch 54/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4380 - val_loss: 0.4425\n",
      "Epoch 55/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4194 - val_loss: 0.4381\n",
      "Epoch 56/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4329 - val_loss: 0.4399\n",
      "Epoch 57/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4279 - val_loss: 0.4442\n",
      "Epoch 58/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4201 - val_loss: 0.4144\n",
      "Epoch 59/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4081 - val_loss: 0.4219\n",
      "Epoch 60/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4128 - val_loss: 0.4194\n",
      "Epoch 61/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4325 - val_loss: 0.4199\n",
      "Epoch 62/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3983 - val_loss: 0.4184\n",
      "Epoch 63/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4029 - val_loss: 0.4188\n",
      "Epoch 64/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4285 - val_loss: 0.4030\n",
      "Epoch 65/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4230 - val_loss: 0.4088\n",
      "Epoch 66/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3978 - val_loss: 0.4090\n",
      "Epoch 67/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4032 - val_loss: 0.4147\n",
      "Epoch 68/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3929 - val_loss: 0.4057\n",
      "Epoch 69/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4065 - val_loss: 0.4147\n",
      "Epoch 70/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3948 - val_loss: 0.4018\n",
      "Epoch 71/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4294 - val_loss: 0.4007\n",
      "Epoch 72/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4128 - val_loss: 0.4013\n",
      "Epoch 73/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4047 - val_loss: 0.4045\n",
      "Epoch 74/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4131 - val_loss: 0.4095\n",
      "Epoch 75/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4050 - val_loss: 0.4034\n",
      "Epoch 76/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3945 - val_loss: 0.4103\n",
      "Epoch 77/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3911 - val_loss: 0.4017\n",
      "Epoch 78/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3823 - val_loss: 0.4024\n",
      "Epoch 79/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4102 - val_loss: 0.3969\n",
      "Epoch 80/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4233 - val_loss: 0.3952\n",
      "Epoch 81/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3922 - val_loss: 0.4114\n",
      "Epoch 82/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4136 - val_loss: 0.4000\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3920 - val_loss: 0.3951\n",
      "Epoch 84/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3797 - val_loss: 0.3951\n",
      "Epoch 85/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3807 - val_loss: 0.3938\n",
      "Epoch 86/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3929 - val_loss: 0.3928\n",
      "Epoch 87/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3902 - val_loss: 0.3930\n",
      "Epoch 88/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3991 - val_loss: 0.3929\n",
      "Epoch 89/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3985 - val_loss: 0.3881\n",
      "Epoch 90/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3787 - val_loss: 0.3909\n",
      "Epoch 91/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3926 - val_loss: 0.3854\n",
      "Epoch 92/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3840 - val_loss: 0.3931\n",
      "Epoch 93/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3719 - val_loss: 0.3914\n",
      "Epoch 94/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3836 - val_loss: 0.3966\n",
      "Epoch 95/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3583 - val_loss: 0.3863\n",
      "Epoch 96/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3868 - val_loss: 0.3933\n",
      "Epoch 97/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3711 - val_loss: 0.3919\n",
      "Epoch 98/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3924 - val_loss: 0.3875\n",
      "Epoch 99/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3822 - val_loss: 0.3749\n",
      "Epoch 100/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3996 - val_loss: 0.3792\n",
      "Epoch 101/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3902 - val_loss: 0.3761\n",
      "Epoch 102/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3761 - val_loss: 0.3653\n",
      "Epoch 103/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3687 - val_loss: 0.3856\n",
      "Epoch 104/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3857 - val_loss: 0.3704\n",
      "Epoch 105/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3890 - val_loss: 0.3728\n",
      "Epoch 106/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3736 - val_loss: 0.3825\n",
      "Epoch 107/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3735 - val_loss: 0.3705\n",
      "Epoch 108/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3672 - val_loss: 0.3642\n",
      "Epoch 109/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3636 - val_loss: 0.3870\n",
      "Epoch 110/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3836 - val_loss: 0.3867\n",
      "Epoch 111/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3742 - val_loss: 0.3807\n",
      "Epoch 112/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3768 - val_loss: 0.3749\n",
      "Epoch 113/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3826 - val_loss: 0.3671\n",
      "Epoch 114/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3689 - val_loss: 0.3660\n",
      "Epoch 115/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3853 - val_loss: 0.3545\n",
      "Epoch 116/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4096 - val_loss: 0.3720\n",
      "Epoch 117/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3664 - val_loss: 0.3666\n",
      "Epoch 118/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3853 - val_loss: 0.3776\n",
      "Epoch 119/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3745 - val_loss: 0.3771\n",
      "Epoch 120/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3869 - val_loss: 0.3655\n",
      "Epoch 121/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3747 - val_loss: 0.3699\n",
      "Epoch 122/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3723 - val_loss: 0.3602\n",
      "Epoch 123/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3782 - val_loss: 0.3569\n",
      "Epoch 124/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3563 - val_loss: 0.3620\n",
      "Epoch 125/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3667 - val_loss: 0.3599\n",
      "Epoch 126/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3612 - val_loss: 0.3780\n",
      "Epoch 127/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3726 - val_loss: 0.3776\n",
      "Epoch 128/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3648 - val_loss: 0.3685\n",
      "Epoch 129/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3726 - val_loss: 0.3676\n",
      "Epoch 130/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3692 - val_loss: 0.3518\n",
      "Epoch 131/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3799 - val_loss: 0.3584\n",
      "Epoch 132/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3679 - val_loss: 0.3663\n",
      "Epoch 133/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3514 - val_loss: 0.3785\n",
      "Epoch 134/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3783 - val_loss: 0.3656\n",
      "Epoch 135/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3613 - val_loss: 0.3498\n",
      "Epoch 136/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3556 - val_loss: 0.3472\n",
      "Epoch 137/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3489 - val_loss: 0.3476\n",
      "Epoch 138/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3696 - val_loss: 0.3481\n",
      "Epoch 139/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3483 - val_loss: 0.3494\n",
      "Epoch 140/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3788 - val_loss: 0.3498\n",
      "Epoch 141/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3811 - val_loss: 0.3506\n",
      "Epoch 142/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3614 - val_loss: 0.3609\n",
      "Epoch 143/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3509 - val_loss: 0.3510\n",
      "Epoch 144/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3483 - val_loss: 0.3592\n",
      "Epoch 145/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3623 - val_loss: 0.3508\n",
      "Epoch 146/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3509\n",
      "Epoch 147/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3779 - val_loss: 0.3473\n",
      "Epoch 148/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3761 - val_loss: 0.3365\n",
      "Epoch 149/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3614 - val_loss: 0.3102\n",
      "Epoch 150/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3594 - val_loss: 0.3293\n",
      "Epoch 151/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3488 - val_loss: 0.3480\n",
      "Epoch 152/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3507 - val_loss: 0.3509\n",
      "Epoch 153/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3487 - val_loss: 0.3382\n",
      "Epoch 154/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3800 - val_loss: 0.3526\n",
      "Epoch 155/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3737 - val_loss: 0.3538\n",
      "Epoch 156/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3608 - val_loss: 0.3562\n",
      "Epoch 157/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3617 - val_loss: 0.3607\n",
      "Epoch 158/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3634 - val_loss: 0.3649\n",
      "Epoch 159/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3413 - val_loss: 0.3504\n",
      "Epoch 160/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3439 - val_loss: 0.3449\n",
      "Epoch 161/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3453 - val_loss: 0.3420\n",
      "Epoch 162/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3493 - val_loss: 0.3406\n",
      "Epoch 163/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3434 - val_loss: 0.3524\n",
      "Epoch 164/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3414 - val_loss: 0.3489\n",
      "Epoch 165/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3549 - val_loss: 0.3489\n",
      "Epoch 166/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3538 - val_loss: 0.3696\n",
      "Epoch 167/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3677 - val_loss: 0.3639\n",
      "Epoch 168/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3633 - val_loss: 0.3389\n",
      "Epoch 169/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3587 - val_loss: 0.3587\n",
      "Epoch 170/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3532 - val_loss: 0.3501\n",
      "Epoch 171/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3579 - val_loss: 0.3544\n",
      "Epoch 172/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3427 - val_loss: 0.3475\n",
      "Epoch 173/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3332 - val_loss: 0.3414\n",
      "Epoch 174/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3356 - val_loss: 0.3406\n",
      "Epoch 175/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3393 - val_loss: 0.3504\n",
      "Epoch 176/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3327 - val_loss: 0.3431\n",
      "Epoch 177/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3394 - val_loss: 0.3639\n",
      "Epoch 178/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3493 - val_loss: 0.3733\n",
      "Epoch 179/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3590 - val_loss: 0.3660\n",
      "Epoch 180/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3411 - val_loss: 0.3588\n",
      "Epoch 181/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3653 - val_loss: 0.3654\n",
      "Epoch 182/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3663 - val_loss: 0.3295\n",
      "Epoch 183/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3592 - val_loss: 0.3533\n",
      "Epoch 184/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3433 - val_loss: 0.3600\n",
      "Epoch 185/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3629 - val_loss: 0.3629\n",
      "Epoch 186/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3559 - val_loss: 0.3483\n",
      "Epoch 187/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3419 - val_loss: 0.3522\n",
      "Epoch 188/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3492 - val_loss: 0.3630\n",
      "Epoch 189/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3531 - val_loss: 0.3696\n",
      "Epoch 190/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.3684\n",
      "Epoch 191/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3447 - val_loss: 0.3618\n",
      "Epoch 192/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3464 - val_loss: 0.3679\n",
      "Epoch 193/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3484 - val_loss: 0.3492\n",
      "Epoch 194/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3334 - val_loss: 0.3512\n",
      "Epoch 195/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3289 - val_loss: 0.3492\n",
      "Epoch 196/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3512 - val_loss: 0.3445\n",
      "Epoch 197/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3373 - val_loss: 0.3447\n",
      "Epoch 198/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3487 - val_loss: 0.3351\n",
      "Epoch 199/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3424 - val_loss: 0.3513\n",
      "Epoch 200/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3410 - val_loss: 0.3516\n",
      "Epoch 201/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3623 - val_loss: 0.3453\n",
      "Epoch 202/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3404 - val_loss: 0.3489\n",
      "Epoch 203/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3424 - val_loss: 0.3574\n",
      "Epoch 204/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3366 - val_loss: 0.3567\n",
      "Epoch 205/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3523 - val_loss: 0.3473\n",
      "Epoch 206/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3509 - val_loss: 0.3363\n",
      "Epoch 207/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3654 - val_loss: 0.3447\n",
      "Epoch 208/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3212 - val_loss: 0.3525\n",
      "Epoch 209/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3506 - val_loss: 0.3459\n",
      "Epoch 210/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3432 - val_loss: 0.3561\n",
      "Epoch 211/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3399 - val_loss: 0.3418\n",
      "Epoch 212/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3435 - val_loss: 0.3414\n",
      "Epoch 213/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3534 - val_loss: 0.3328\n",
      "Epoch 214/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3366 - val_loss: 0.3445\n",
      "Epoch 215/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3452 - val_loss: 0.3335\n",
      "Epoch 216/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3473 - val_loss: 0.3330\n",
      "Epoch 217/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3415 - val_loss: 0.3390\n",
      "Epoch 218/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3557 - val_loss: 0.3333\n",
      "Epoch 219/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3522 - val_loss: 0.3406\n",
      "Epoch 220/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3546 - val_loss: 0.3543\n",
      "Epoch 221/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3421 - val_loss: 0.3688\n",
      "Epoch 222/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3325 - val_loss: 0.3603\n",
      "Epoch 223/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3379 - val_loss: 0.3538\n",
      "Epoch 224/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3220 - val_loss: 0.3502\n",
      "Epoch 225/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3228 - val_loss: 0.3497\n",
      "Epoch 226/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3498 - val_loss: 0.3416\n",
      "Epoch 227/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3420 - val_loss: 0.3439\n",
      "Epoch 228/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3640 - val_loss: 0.3455\n",
      "Epoch 229/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3411 - val_loss: 0.3472\n",
      "Epoch 230/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3549 - val_loss: 0.3477\n",
      "Epoch 231/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3453 - val_loss: 0.3415\n",
      "Epoch 232/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3497 - val_loss: 0.3421\n",
      "Epoch 233/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3377 - val_loss: 0.3464\n",
      "Epoch 234/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3479 - val_loss: 0.3458\n",
      "Epoch 235/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3338 - val_loss: 0.3444\n",
      "Epoch 236/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3350 - val_loss: 0.3391\n",
      "Epoch 237/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3346 - val_loss: 0.3478\n",
      "Epoch 238/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3425 - val_loss: 0.3548\n",
      "Epoch 239/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3425 - val_loss: 0.3549\n",
      "Epoch 240/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3428 - val_loss: 0.3491\n",
      "Epoch 241/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3351 - val_loss: 0.3339\n",
      "Epoch 242/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3414 - val_loss: 0.3450\n",
      "Epoch 243/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3366 - val_loss: 0.3508\n",
      "Epoch 244/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3323 - val_loss: 0.3459\n",
      "Epoch 245/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3457 - val_loss: 0.3442\n",
      "Epoch 246/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3328 - val_loss: 0.3450\n",
      "Epoch 247/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3266 - val_loss: 0.3569\n",
      "Epoch 248/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3306 - val_loss: 0.3489\n",
      "Epoch 249/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3368 - val_loss: 0.3538\n",
      "Epoch 250/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3235 - val_loss: 0.3450\n",
      "Epoch 251/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3224 - val_loss: 0.3485\n",
      "Epoch 252/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3347 - val_loss: 0.3577\n",
      "Epoch 253/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3310 - val_loss: 0.3599\n",
      "Epoch 254/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3327 - val_loss: 0.3459\n",
      "Epoch 255/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3249 - val_loss: 0.3537\n",
      "Epoch 256/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3370 - val_loss: 0.3535\n",
      "Epoch 257/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3280 - val_loss: 0.3496\n",
      "Epoch 258/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3282 - val_loss: 0.3428\n",
      "Epoch 259/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3256 - val_loss: 0.3505\n",
      "Epoch 260/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3266 - val_loss: 0.3261\n",
      "Epoch 261/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3132 - val_loss: 0.3386\n",
      "Epoch 262/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3209 - val_loss: 0.3316\n",
      "Epoch 263/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3298 - val_loss: 0.3384\n",
      "Epoch 264/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3388 - val_loss: 0.3362\n",
      "Epoch 265/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3343 - val_loss: 0.3385\n",
      "Epoch 266/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3213 - val_loss: 0.3375\n",
      "Epoch 267/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3409 - val_loss: 0.3405\n",
      "Epoch 268/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3311 - val_loss: 0.3409\n",
      "Epoch 269/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3341 - val_loss: 0.3399\n",
      "Epoch 270/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3290 - val_loss: 0.3515\n",
      "Epoch 271/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3347 - val_loss: 0.3516\n",
      "Epoch 272/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3377 - val_loss: 0.3409\n",
      "Epoch 273/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3208 - val_loss: 0.3298\n",
      "Epoch 274/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3597 - val_loss: 0.3516\n",
      "Epoch 275/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3362 - val_loss: 0.3508\n",
      "Epoch 276/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3370 - val_loss: 0.3408\n",
      "Epoch 277/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3429\n",
      "Epoch 278/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3557 - val_loss: 0.3500\n",
      "Epoch 279/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3327 - val_loss: 0.3458\n",
      "Epoch 280/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3453 - val_loss: 0.3452\n",
      "Epoch 281/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3484 - val_loss: 0.3354\n",
      "Epoch 282/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3460 - val_loss: 0.3292\n",
      "Epoch 283/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3409 - val_loss: 0.3382\n",
      "Epoch 284/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3170 - val_loss: 0.3549\n",
      "Epoch 285/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3204 - val_loss: 0.3448\n",
      "Epoch 286/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3358 - val_loss: 0.3291\n",
      "Epoch 287/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3319 - val_loss: 0.3385\n",
      "Epoch 288/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3361 - val_loss: 0.3409\n",
      "Epoch 289/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3402 - val_loss: 0.3307\n",
      "Epoch 290/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3383 - val_loss: 0.3258\n",
      "Epoch 291/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3339 - val_loss: 0.3394\n",
      "Epoch 292/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3458 - val_loss: 0.3437\n",
      "Epoch 293/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3369 - val_loss: 0.3511\n",
      "Epoch 294/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3415 - val_loss: 0.3431\n",
      "Epoch 295/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3436 - val_loss: 0.3462\n",
      "Epoch 296/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3283 - val_loss: 0.3465\n",
      "Epoch 297/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3353 - val_loss: 0.3613\n",
      "Epoch 298/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3392 - val_loss: 0.3458\n",
      "Epoch 299/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3175 - val_loss: 0.3387\n",
      "Epoch 300/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3461 - val_loss: 0.3516\n",
      "Epoch 301/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3353 - val_loss: 0.3570\n",
      "Epoch 302/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3088 - val_loss: 0.3612\n",
      "Epoch 303/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3189 - val_loss: 0.3368\n",
      "Epoch 304/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3469 - val_loss: 0.3407\n",
      "Epoch 305/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3531 - val_loss: 0.3530\n",
      "Epoch 306/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3148 - val_loss: 0.3604\n",
      "Epoch 307/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3304 - val_loss: 0.3455\n",
      "Epoch 308/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3318 - val_loss: 0.3343\n",
      "Epoch 309/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3143 - val_loss: 0.3477\n",
      "Epoch 310/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3239 - val_loss: 0.3421\n",
      "Epoch 311/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3253 - val_loss: 0.3340\n",
      "Epoch 312/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3440 - val_loss: 0.3419\n",
      "Epoch 313/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3186 - val_loss: 0.3455\n",
      "Epoch 314/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3357 - val_loss: 0.3325\n",
      "Epoch 315/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3253 - val_loss: 0.3329\n",
      "Epoch 316/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3204 - val_loss: 0.3387\n",
      "Epoch 317/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3133 - val_loss: 0.3399\n",
      "Epoch 318/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3270 - val_loss: 0.3485\n",
      "Epoch 319/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3273 - val_loss: 0.3464\n",
      "Epoch 320/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2968 - val_loss: 0.3338\n",
      "Epoch 321/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3277 - val_loss: 0.3364\n",
      "Epoch 322/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3354 - val_loss: 0.3747\n",
      "Epoch 323/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3246 - val_loss: 0.3641\n",
      "Epoch 324/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3269 - val_loss: 0.3503\n",
      "Epoch 325/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3349 - val_loss: 0.3401\n",
      "Epoch 326/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3136 - val_loss: 0.3503\n",
      "Epoch 327/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3312 - val_loss: 0.3412\n",
      "Epoch 328/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3315 - val_loss: 0.3485\n",
      "Epoch 329/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3419 - val_loss: 0.3453\n",
      "Epoch 330/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3383 - val_loss: 0.3435\n",
      "Epoch 331/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3164 - val_loss: 0.3567\n",
      "Epoch 332/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3705\n",
      "Epoch 333/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3452 - val_loss: 0.3475\n",
      "Epoch 334/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3279 - val_loss: 0.3464\n",
      "Epoch 335/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3424 - val_loss: 0.3514\n",
      "Epoch 336/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3232 - val_loss: 0.3437\n",
      "Epoch 337/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3271 - val_loss: 0.3461\n",
      "Epoch 338/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3200 - val_loss: 0.3450\n",
      "Epoch 339/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3235 - val_loss: 0.3447\n",
      "Epoch 340/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3302 - val_loss: 0.3401\n",
      "Epoch 341/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3375 - val_loss: 0.3339\n",
      "Epoch 342/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3240 - val_loss: 0.3314\n",
      "Epoch 343/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3368 - val_loss: 0.3292\n",
      "Epoch 344/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3340 - val_loss: 0.3279\n",
      "Epoch 345/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3227 - val_loss: 0.3374\n",
      "Epoch 346/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3159 - val_loss: 0.3392\n",
      "Epoch 347/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3228 - val_loss: 0.3470\n",
      "Epoch 348/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3139 - val_loss: 0.3406\n",
      "Epoch 349/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3104 - val_loss: 0.3361\n",
      "Epoch 350/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3387 - val_loss: 0.3496\n",
      "Epoch 351/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3034 - val_loss: 0.3439\n",
      "Epoch 352/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3225 - val_loss: 0.3294\n",
      "Epoch 353/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3173 - val_loss: 0.3502\n",
      "Epoch 354/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3094 - val_loss: 0.3503\n",
      "Epoch 355/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3031 - val_loss: 0.3453\n",
      "Epoch 356/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3172 - val_loss: 0.3503\n",
      "Epoch 357/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3364 - val_loss: 0.3383\n",
      "Epoch 358/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3089 - val_loss: 0.3279\n",
      "Epoch 359/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3336 - val_loss: 0.3424\n",
      "Epoch 360/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3274 - val_loss: 0.3475\n",
      "Epoch 361/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3176 - val_loss: 0.3300\n",
      "Epoch 362/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3302 - val_loss: 0.3320\n",
      "Epoch 363/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3141 - val_loss: 0.3467\n",
      "Epoch 364/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3361 - val_loss: 0.3297\n",
      "Epoch 365/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3375 - val_loss: 0.3442\n",
      "Epoch 366/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3173 - val_loss: 0.3546\n",
      "Epoch 367/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3165 - val_loss: 0.3370\n",
      "Epoch 368/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3498 - val_loss: 0.3472\n",
      "Epoch 369/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3352 - val_loss: 0.3389\n",
      "Epoch 370/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3260 - val_loss: 0.3427\n",
      "Epoch 371/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3373 - val_loss: 0.3473\n",
      "Epoch 372/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3381 - val_loss: 0.3347\n",
      "Epoch 373/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3248 - val_loss: 0.3285\n",
      "Epoch 374/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3366 - val_loss: 0.3351\n",
      "Epoch 375/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3105 - val_loss: 0.3401\n",
      "Epoch 376/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3294 - val_loss: 0.3270\n",
      "Epoch 377/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3101 - val_loss: 0.3265\n",
      "Epoch 378/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3344 - val_loss: 0.3459\n",
      "Epoch 379/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3283 - val_loss: 0.3462\n",
      "Epoch 380/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3148 - val_loss: 0.3303\n",
      "Epoch 381/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3208 - val_loss: 0.3451\n",
      "Epoch 382/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3340 - val_loss: 0.3265\n",
      "Epoch 383/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3364 - val_loss: 0.3154\n",
      "Epoch 384/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3309 - val_loss: 0.3392\n",
      "Epoch 385/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3162 - val_loss: 0.3382\n",
      "Epoch 386/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3279 - val_loss: 0.3416\n",
      "Epoch 387/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3174 - val_loss: 0.3354\n",
      "Epoch 388/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3144 - val_loss: 0.3342\n",
      "Epoch 389/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3298 - val_loss: 0.3283\n",
      "Epoch 390/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3057 - val_loss: 0.3215\n",
      "Epoch 391/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3243 - val_loss: 0.3235\n",
      "Epoch 392/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3365 - val_loss: 0.3351\n",
      "Epoch 393/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3386 - val_loss: 0.3401\n",
      "Epoch 394/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3432 - val_loss: 0.3776\n",
      "Epoch 395/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3269 - val_loss: 0.3794\n",
      "Epoch 396/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3464 - val_loss: 0.3760\n",
      "Epoch 397/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3356 - val_loss: 0.3768\n",
      "Epoch 398/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3318 - val_loss: 0.3731\n",
      "Epoch 399/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3145 - val_loss: 0.3682\n",
      "Epoch 400/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3422 - val_loss: 0.3625\n",
      "Epoch 401/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3162 - val_loss: 0.3592\n",
      "Epoch 402/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3542 - val_loss: 0.3609\n",
      "Epoch 403/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3355 - val_loss: 0.3641\n",
      "Epoch 404/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3378 - val_loss: 0.3444\n",
      "Epoch 405/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3295 - val_loss: 0.3403\n",
      "Epoch 406/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3252 - val_loss: 0.3456\n",
      "Epoch 407/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3459 - val_loss: 0.3628\n",
      "Epoch 408/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3281 - val_loss: 0.3483\n",
      "Epoch 409/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3241 - val_loss: 0.3300\n",
      "Epoch 410/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3338 - val_loss: 0.3354\n",
      "Epoch 411/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3328 - val_loss: 0.3263\n",
      "Epoch 412/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3280 - val_loss: 0.3226\n",
      "Epoch 413/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3160 - val_loss: 0.3312\n",
      "Epoch 414/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3303 - val_loss: 0.3239\n",
      "Epoch 415/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3272 - val_loss: 0.3279\n",
      "Epoch 416/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3096 - val_loss: 0.3383\n",
      "Epoch 417/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3141 - val_loss: 0.3402\n",
      "Epoch 418/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3266 - val_loss: 0.3335\n",
      "Epoch 419/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3114 - val_loss: 0.3198\n",
      "Epoch 420/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3320 - val_loss: 0.3239\n",
      "Epoch 421/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3117 - val_loss: 0.3296\n",
      "Epoch 422/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3239 - val_loss: 0.3226\n",
      "Epoch 423/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3279 - val_loss: 0.3619\n",
      "Epoch 424/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3235 - val_loss: 0.3609\n",
      "Epoch 425/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3265 - val_loss: 0.3575\n",
      "Epoch 426/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3335 - val_loss: 0.3471\n",
      "Epoch 427/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3349 - val_loss: 0.3393\n",
      "Epoch 428/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3223 - val_loss: 0.3351\n",
      "Epoch 429/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3092 - val_loss: 0.3328\n",
      "Epoch 430/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3056 - val_loss: 0.3371\n",
      "Epoch 431/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3005 - val_loss: 0.3418\n",
      "Epoch 432/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3272 - val_loss: 0.3236\n",
      "Epoch 433/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3103 - val_loss: 0.3225\n",
      "Epoch 434/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3244 - val_loss: 0.3310\n",
      "Epoch 435/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3214 - val_loss: 0.3202\n",
      "Epoch 436/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3038 - val_loss: 0.3209\n",
      "Epoch 437/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3155 - val_loss: 0.3257\n",
      "Epoch 438/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3164 - val_loss: 0.3167\n",
      "Epoch 439/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3266 - val_loss: 0.3281\n",
      "Epoch 440/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3237 - val_loss: 0.3242\n",
      "Epoch 441/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3151 - val_loss: 0.3266\n",
      "Epoch 442/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3102 - val_loss: 0.3322\n",
      "Epoch 443/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3274 - val_loss: 0.3281\n",
      "Epoch 444/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3062 - val_loss: 0.3194\n",
      "Epoch 445/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3309 - val_loss: 0.3330\n",
      "Epoch 446/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3451 - val_loss: 0.3305\n",
      "Epoch 447/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3078 - val_loss: 0.3370\n",
      "Epoch 448/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3295 - val_loss: 0.3482\n",
      "Epoch 449/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3041 - val_loss: 0.3328\n",
      "Epoch 450/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3119 - val_loss: 0.3204\n",
      "Epoch 451/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3160 - val_loss: 0.3207\n",
      "Epoch 452/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2999 - val_loss: 0.3194\n",
      "Epoch 453/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3037 - val_loss: 0.3306\n",
      "Epoch 454/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3169 - val_loss: 0.3281\n",
      "Epoch 455/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3064 - val_loss: 0.3339\n",
      "Epoch 456/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3124 - val_loss: 0.3384\n",
      "Epoch 457/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3269 - val_loss: 0.3337\n",
      "Epoch 458/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3023 - val_loss: 0.3340\n",
      "Epoch 459/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3115 - val_loss: 0.3319\n",
      "Epoch 460/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2980 - val_loss: 0.3259\n",
      "Epoch 461/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3348 - val_loss: 0.3302\n",
      "Epoch 462/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3115 - val_loss: 0.3109\n",
      "Epoch 463/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3255 - val_loss: 0.3241\n",
      "Epoch 464/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3222 - val_loss: 0.3292\n",
      "Epoch 465/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3301 - val_loss: 0.3175\n",
      "Epoch 466/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3222 - val_loss: 0.3072\n",
      "Epoch 467/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3169 - val_loss: 0.3323\n",
      "Epoch 468/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3255 - val_loss: 0.3210\n",
      "Epoch 469/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3151 - val_loss: 0.3108\n",
      "Epoch 470/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3255 - val_loss: 0.3133\n",
      "Epoch 471/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3196 - val_loss: 0.3249\n",
      "Epoch 472/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3158 - val_loss: 0.3153\n",
      "Epoch 473/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3083 - val_loss: 0.3067\n",
      "Epoch 474/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3441 - val_loss: 0.3193\n",
      "Epoch 475/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2977 - val_loss: 0.3118\n",
      "Epoch 476/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3115 - val_loss: 0.3199\n",
      "Epoch 477/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3319 - val_loss: 0.3160\n",
      "Epoch 478/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3183 - val_loss: 0.3162\n",
      "Epoch 479/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3225 - val_loss: 0.3262\n",
      "Epoch 480/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3102 - val_loss: 0.3313\n",
      "Epoch 481/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3211 - val_loss: 0.3249\n",
      "Epoch 482/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3049 - val_loss: 0.3162\n",
      "Epoch 483/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3155\n",
      "Epoch 484/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3136 - val_loss: 0.3063\n",
      "Epoch 485/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3117 - val_loss: 0.3141\n",
      "Epoch 486/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3234 - val_loss: 0.3212\n",
      "Epoch 487/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3063 - val_loss: 0.3156\n",
      "Epoch 488/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3165 - val_loss: 0.3307\n",
      "Epoch 489/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3320 - val_loss: 0.3224\n",
      "Epoch 490/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3303 - val_loss: 0.3210\n",
      "Epoch 491/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3194 - val_loss: 0.3348\n",
      "Epoch 492/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3321 - val_loss: 0.3224\n",
      "Epoch 493/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3194 - val_loss: 0.3138\n",
      "Epoch 494/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3114 - val_loss: 0.3239\n",
      "Epoch 495/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3140 - val_loss: 0.3198\n",
      "Epoch 496/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3122 - val_loss: 0.3235\n",
      "Epoch 497/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3179 - val_loss: 0.3292\n",
      "Epoch 498/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3172 - val_loss: 0.3206\n",
      "Epoch 499/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3125 - val_loss: 0.3155\n",
      "Epoch 500/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3181 - val_loss: 0.3068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8cdaa91d60>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train[features], y_train, validation_data = (x_val[features], y_val),\n",
    "         epochs = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7814569536423841,\n",
       " liked    0.781457\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(x_val[features])\n",
    "(pred == y_val.values).mean(), y_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7988668555240793,\n",
       " liked    0.798867\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(x_train[features])\n",
    "(pred == y_train.values).mean(), y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_atr import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = PodcastRecommendation(uri, (user, psw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pr.gen_df('6C561484AED5C02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pr.gen_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47    b4c3c3ebdd76e284f7d9fa358ac82030\n",
       "17    c9add5e9e81a4b3ca963adab5b87083f\n",
       "7     a37fb116709bfdb2dd58ea4f784cb815\n",
       "42    a3a535f66c7e8004e7dc54c2b2829a9e\n",
       "43    b70d658c901897359bb848cf876cbcbc\n",
       "14    f9938eec02c773585575a091fe35c425\n",
       "2     d6ef1df72820e4fc2591f8c3a295b1b7\n",
       "6     c4b4a343d1131676bf592d266629d6dc\n",
       "18    d2042758dcacc179d98a76d7673065ac\n",
       "13    bd1eeaa544589b34fe3056a7a60d9360\n",
       "4     a627d4945f6fcfc2c9050233cbb5369a\n",
       "16    bffe9d51afd8a6c6a89c7cbb06da788f\n",
       "1     f81c8201b9e4f47961987e287ac54cc0\n",
       "3     db45e4be97084c8d133d88cee1c97897\n",
       "20    b61de95fa84608350836ed76b0243eef\n",
       "12    d2da82feac2f430eb12a29a5ca22d0c5\n",
       "48    a5416c179e94a74e61827c88e405b083\n",
       "25    f42b4160ff9b8933d74c411ee40596ea\n",
       "5     a145b05c1f310d835b6086fdb03493a9\n",
       "44    fa3cac03e05c1e9ec792e95e4850d7ce\n",
       "41    c1adb6ca5ca39575420fda03c099b037\n",
       "49    bda31244e9514743575bfd63b1c2aa6e\n",
       "8     ed1d26a2f16587ec292421e188f8ee83\n",
       "45    d87a09dc72d2914ab0ed9591d13b5ff3\n",
       "29    e853203063767cd03182118755f515e5\n",
       "46    abbdecf5e1cde3bb520c8e413a74d4ec\n",
       "0     d58d098f5227ba67fefb62673aa1d854\n",
       "11    b6e2e0dcd8ad43c68e2a7b2314dd2bbd\n",
       "10    f2f6a2352b3b15c23543e1f5e24747de\n",
       "21    e657eef8b576e7a8f023e2b38699637f\n",
       "37    bb9ba896707ab780a0a0956b7b2ded09\n",
       "34    b3357f42118f68bab994ae672fcdd4ed\n",
       "26    eaf0048c493d201c40cc1850d78196e8\n",
       "15    f529487b091b7659049b17420ae20267\n",
       "19    fe2f0190ec3d970fec5a5196fed572c9\n",
       "9     e7d6c4bdffba98ea2267f91a9a9a2763\n",
       "40    a25e950f1530a111b4d83b4d27b4f351\n",
       "38    e28892fd142248a15b9d46b30ebd9d38\n",
       "36    e6568415fd1b0c32cbefbb4cb6dc78d2\n",
       "23    caba7b32181b486496ff69dc9a49d2f3\n",
       "24    c54d17cdabe329c9af0e6a2e288fba41\n",
       "22    b69677f9f694bd3566eaf9fff0343784\n",
       "28    dbff7172efd9adb50e02f573f43f983f\n",
       "39    c787f026dea80607a984c6693bb07e1c\n",
       "35    c0deaddd87c334ed4ae858f3aa358943\n",
       "33    f4611ddbb62e95ce0a287bfd6da597de\n",
       "32    c0a88c0d4805cf60bb90ca34337438f4\n",
       "31    a4203a8f44a37a25a57a632603edd5d7\n",
       "30    ec41f6b4cf6f92299f29e29ed9fa6b75\n",
       "27    f034c2c035198bcfd781907627876430\n",
       "Name: podcast_id, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.recommend('6C561484AED5C02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
